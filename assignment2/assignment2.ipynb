{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MJM\\OneDrive\\桌面\\text mining\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")  # Adjust the model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iob_data(filepath):\n",
    "    tokens, labels = [], []\n",
    "    sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check for blank line (indicates end of sentence)\n",
    "            if not line:\n",
    "                if sentence_tokens:\n",
    "                    tokens.append(sentence_tokens)\n",
    "                    labels.append(sentence_labels)\n",
    "                    sentence_tokens, sentence_labels = [], []\n",
    "            else:\n",
    "                # Token format: word POS_tag IOB_label\n",
    "                parts = line.split()\n",
    "                if len(parts) == 3:\n",
    "                    token, pos_tag, label = parts\n",
    "                    sentence_tokens.append(token)\n",
    "                    sentence_labels.append(label)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected format in line: {line}\")\n",
    "                    \n",
    "    # Add the last sentence if it exists\n",
    "    if sentence_tokens:\n",
    "        tokens.append(sentence_tokens)\n",
    "        labels.append(sentence_labels)\n",
    "    \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1992 sentences for training.\n",
      "Loaded 850 sentences for validation.\n",
      "Loaded 864 sentences for testing.\n"
     ]
    }
   ],
   "source": [
    "# Load each data split\n",
    "train_tokens, train_labels = load_iob_data(\"fold1/train.txt\")\n",
    "val_tokens, val_labels = load_iob_data(\"fold1/val.txt\")\n",
    "test_tokens, test_labels = load_iob_data(\"fold1/test.txt\")\n",
    "\n",
    "print(f\"Loaded {len(train_tokens)} sentences for training.\")\n",
    "print(f\"Loaded {len(val_tokens)} sentences for validation.\")\n",
    "print(f\"Loaded {len(test_tokens)} sentences for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_to_id: {'B-ART': 0, 'B-CON': 1, 'B-LOC': 2, 'B-MAT': 3, 'B-PER': 4, 'B-SPE': 5, 'I-ART': 6, 'I-CON': 7, 'I-LOC': 8, 'I-MAT': 9, 'I-PER': 10, 'I-SPE': 11, 'O': 12}\n",
      "id_to_label: {0: 'B-ART', 1: 'B-CON', 2: 'B-LOC', 3: 'B-MAT', 4: 'B-PER', 5: 'B-SPE', 6: 'I-ART', 7: 'I-CON', 8: 'I-LOC', 9: 'I-MAT', 10: 'I-PER', 11: 'I-SPE', 12: 'O'}\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique labels and a label-to-id mapping\n",
    "unique_labels = set(label for sentence_labels in train_labels + val_labels + test_labels for label in sentence_labels)\n",
    "label_to_id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"label_to_id:\", label_to_id)\n",
    "print(\"id_to_label:\", id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and align labels with tokenized data\n",
    "def tokenize_and_align_labels(tokens, labels):\n",
    "    tokenized_inputs = tokenizer(tokens, truncation=True, is_split_into_words=True)\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to original word IDs\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        if word_ids is None:\n",
    "            print(f\"Warning: No word_ids generated for sentence {i}. Tokens: {tokens[i]}\")\n",
    "            continue\n",
    "\n",
    "        # Align each token's label\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignore special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id.get(label[word_idx], -100))  # Convert label to ID\n",
    "            else:\n",
    "                label_ids.append(-100)  # Ignore sub-tokens\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    if len(aligned_labels) != len(tokenized_inputs[\"input_ids\"]):\n",
    "        print(f\"Mismatch in length for tokens and labels. Tokens: {len(tokenized_inputs['input_ids'])}, Labels: {len(aligned_labels)}\")\n",
    "        raise ValueError(f\"Mismatch in length for tokens and labels: {len(tokenized_inputs['input_ids'])} vs {len(aligned_labels)}\")\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 15982, 1407, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [-100, 12, 12, 12, -100]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and align the datasets\n",
    "train_data = tokenize_and_align_labels(train_tokens, train_labels)\n",
    "val_data = tokenize_and_align_labels(val_tokens, val_labels)\n",
    "test_data = tokenize_and_align_labels(test_tokens, test_labels)\n",
    "\n",
    "# Convert tokenized data into Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Display dataset structure for verification\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1992\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 850\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 864\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
